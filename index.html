<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Pengfei  He</title>
    <meta name="author" content="Pengfei  He">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://pengfeihepower.github.io/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item active">
                <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/Publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/TA&amp;mentor/">TA&amp;mentor</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- about.html -->
      <div class="post">
        <header class="post-header">
          <h1 class="post-title">
           <span class="font-weight-bold">Pengfei</span>  He
          </h1>
          <p class="desc"></p>
        </header>

        <article>
          <div class="profile float-right">

              <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/prof_pic.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="prof_pic.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

            <div class="address">
              <p>428 S Shaw Ln Rm 3308</p> <p>East Lansing, MI, 48824</p>

            </div>
          </div>

          <div class="clearfix">
            <p>I am a PhD student major in Computer Science and Engineering and minor in Probability and Statistics, at Michigan State University.</p>

<p>My research interests are robustness and safety of machine learning models, optimization and machine learning foundations. Currently, I am interested in <strong>Trustworthy LLMs and Agents</strong>. Specifically,  I am working on <strong>revealing</strong> and <strong>mitigating</strong> vulnerablities of <strong>LLM agents</strong>; understanding and improving <strong>reasoning</strong> and <strong>tool learning</strong> capabilities of LLMs.
Looking forward to communicating with people from different fields!</p>

<p>For AGI and Security!</p>

<!-- Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

          </div>

          <!-- News -->
          <h2><a href="/news/" style="color: inherit;">news</a></h2>          <div class="news">
            <div class="table-responsive" style="max-height: 40vw">
              <table class="table table-sm table-borderless">
              
                <tr>
                  <th scope="row">Aug 5, 2025</th>
                  <td>
                    My work <strong>Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems</strong> [https://arxiv.org/pdf/2506.02546] will be presented on USENIX Security ‘25!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Aug 3, 2025</th>
                  <td>
                    I am proud to give an invoted talk on JSM 2025 about our work on understanding in-context learning by the lens of Bayesian inferece [https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.70045].

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 25, 2025</th>
                  <td>
                    Four papers accepted to ACL 2025! Include <a href="https://arxiv.org/pdf/2502.14847" rel="external nofollow noopener" target="_blank">multi-agent safety</a>, <a href="https://arxiv.org/pdf/2502.13172" rel="external nofollow noopener" target="_blank">agent memoory privacy</a>, <a href="https://arxiv.org/pdf/2502.13260" rel="external nofollow noopener" target="_blank">LLM reasonin</a> and <a href="https://arxiv.org/pdf/2502.14100" rel="external nofollow noopener" target="_blank">RAG robustness</a>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 19, 2025</th>
                  <td>
                    I am proud to be listed as a <strong>notable reviewer</strong> for ICLR 2025.

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 1, 2025</th>
                  <td>
                    Proud to share our position paper, <a href="https://arxiv.org/pdf/2502.14182?" rel="external nofollow noopener" target="_blank">Multi-Faceted Studies on Data Poisoning can Advance LLM Development</a>. In this work, we summarize existing reseaches on data poisoning attacks on LLM, and point out two key limitations. To expand the scope of data poisoning, we propose two novel perspectives: trust-centric and mechanism-centric, to push the study of data poisoning into a new era.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 22, 2025</th>
                  <td>
                    Our work <strong>Data poisoning for in-context learning</strong> is accepted to NAACL 2025!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 19, 2025</th>
                  <td>
                    Our work <strong>Superiority of multi-head attention in in-context linear regression</strong> and <strong>A theoretical understanding of chain-of-thought: Coherent reasoning and error-aware demonstration</strong> is accepted to AISTATS!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 19, 2025</th>
                  <td>
                    Our work <strong>Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</strong> is accepted to Stat(Special Issue on Statistics for Large Language Models and Large Language Models for Statistics)!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Nov 8, 2024</th>
                  <td>
                    Our work <strong>Stealthy Backdoor Attack via Confidence-driven Sampling</strong> is accepted to TMLR!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 21, 2024</th>
                  <td>
                    We preprint a new work <strong>A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration</strong>!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 14, 2024</th>
                  <td>
                    We preprint a new work <strong>Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</strong>!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 10, 2024</th>
                  <td>
                    One paper(<strong>Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</strong>) is accepted to M3L and SFLLM NeruIPS 2024!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 1, 2024</th>
                  <td>
                    I will serve as the reviewer for ICLR 2025 and AISTAT 2025.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Sep 20, 2024</th>
                  <td>
                    Two papers(	
<strong>Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis; On the generalization of training-based chatgpt detection methods</strong>) accepted to EMNLP 2024!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jun 3, 2024</th>
                  <td>
                    I start a new position as Research Intern at Alibaba Group(US) in Bellevue, WA.

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 16, 2024</th>
                  <td>
                    We have two papers(	
<strong>Exploring Memorization in Fine-tuned Language Models; The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation (RAG)</strong>) accepted to <strong>ACL2024</strong>!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Feb 6, 2024</th>
                  <td>
                    We preprint a paper: <strong>Data Poisoning for In-context Learning</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row">Feb 6, 2024</th>
                  <td>
                    We preprint a paper: <strong>Superiority of Multi-Head Attention in In-Context Linear Regression</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row">Feb 5, 2024</th>
                  <td>
                    We release our survey paper about copyright: <strong>Copyright Protection in Generative AI: A Technical Perspective</strong>

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jan 16, 2024</th>
                  <td>
                    Our paper: <strong>Sharpness-aware Data Poisoning Attack</strong> is accepted as <strong>Spotlight (5%)</strong> by ICLR2024!

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 11, 2023</th>
                  <td>
                    We preprint a paper: <strong>Exploring Memorization in Fine-tuned Language Models</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 10, 2023</th>
                  <td>
                    We preprint a paper: <strong>On the Generalization of Training-based ChatGPT Detection Methods</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 10, 2023</th>
                  <td>
                    We preprint a paper: <strong>FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Oct 9, 2023</th>
                  <td>
                    We preprint a paper: <strong>Confidence-driven Sampling for Backdoor Attacks</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Sep 8, 2023</th>
                  <td>
                    Our paper <strong>Analyzing Illegal Psychostimulant Trafficking Networks Using Noisy and Sparse Data</strong> is on IISE Transactions now.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jul 22, 2023</th>
                  <td>
                    I will serve as an external reviewer for ICDM 2023.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jul 13, 2023</th>
                  <td>
                    I will serve as the PC member of AAAI’24.

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 25, 2023</th>
                  <td>
                    We preprint a paper: <strong>DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">May 24, 2023</th>
                  <td>
                    We preprint a paper: <strong>Sharpness-aware Data Poisoning Attack</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Apr 24, 2023</th>
                  <td>
                    Our paper <strong>Probabilistic Categorical Adversarial Attack &amp; Adversarial Training</strong> is accepted to <em>ICML2023</em>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Apr 20, 2023</th>
                  <td>
                    Our paper <strong>Large sample
spectral analysis of graph-based multi-manifold clustering</strong> is accepted to <em>Journal of Machine Learning Research</em>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Dec 29, 2022</th>
                  <td>
                    I will serve as the PC member of KDD’23.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Sep 28, 2022</th>
                  <td>
                    We preprint a paper: <strong>Probabilistic Categorical Adversarial Attack &amp; Adversarial Training</strong>.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Aug 15, 2022</th>
                  <td>
                    We hold a lecture-style tutorial about Adversarial Robustness and Poisoning Attacks in the KDD 2022.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Aug 10, 2022</th>
                  <td>
                    I will serve as the PC member of AAAI’23.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Aug 1, 2022</th>
                  <td>
                    Our paper <strong>PROPN: Personalized Probabilistic Strategic Parameter Optimization in Recommendations</strong> got accepted to CIKM’22.

                  </td>
                </tr>
                <tr>
                  <th scope="row">Jul 14, 2021</th>
                  <td>
                    We preprint a paper: <strong>Large sample spectral analysis of graph-based multi-manifold clustering</strong>.

                  </td>
                </tr>
              </table>
            </div>
          </div>

          <!-- Latest posts -->
          

          <!-- Selected papers -->
          <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2>
          <div class="publications">
            <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">JMLR</abbr></div>

        <!-- Entry bib key -->
        <div id="JMLR:v24:21-1254" class="col-sm-8">
        <!-- Title -->
        <div class="title">Large sample spectral analysis of graph-based multi-manifold clustering</div>
        <!-- Author -->
        <div class="author">
        

        Nicolas Garcia Trillos*, <em>Pengfei He*</em>, and Chenghui Li*</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Journal of Machine Learning Research (JMLR)</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://jmlr.org/papers/v24/21-1254.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In this work we study statistical properties of graph-based algorithms for multi-manifold
clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a
given Euclidean data set when this one is assumed to be obtained by sampling a distribution
on a union of manifolds M = M1 ∪ · · · ∪ MN that may intersect with each other and
that may have different dimensions. We investigate sufficient conditions that similarity
graphs on data sets must satisfy in order for their corresponding graph Laplacians to
capture the right geometric information to solve the MMC problem. Precisely, we provide
high probability error bounds for the spectral approximation of a tensorized Laplacian on
M with a suitable graph Laplacian built from the observations; the recovered tensorized
Laplacian contains all geometric information of all the individual underlying manifolds. We
provide an example of a family of similarity graphs, which we call annular proximity graphs
with angle constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of tangent planes.
Extensive numerical experiments expand the insights that our theory provides on the MMC
problem.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ICLR Spotlight</abbr></div>

        <!-- Entry bib key -->
        <div id="he2023sharpness" class="col-sm-8">
        <!-- Title -->
        <div class="title">Sharpness-Aware Data Poisoning Attack</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Han Xu, Jie Ren, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Yingqian Cui, Hui Liu, Charu C Aggarwal, Jiliang Tang' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In International Conference on Learning Representations (ICLR)</em>, 2024
        </div>
        <div class="periodical">
          Spotlight Paper, 5%
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2305.14851" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs)
against data poisoning attacks. These attacks aim to inject poisoning samples into
the models’ training dataset such that the trained models have inference failures.
While previous studies have executed different types of attacks, one major challenge
that greatly limits their effectiveness is the uncertainty of the re-training process
after the injection of poisoning samples, including the re-training initialization or
algorithms. To address this challenge, we propose a novel attack method called
“Sharpness-Aware Data Poisoning Attack (SAPA)”. In particular, it leverages the
concept of DNNs’ loss landscape sharpness to optimize the poisoning effect on the
worst re-trained model. It helps enhance the preservation of the poisoning effect,
regardless of the specific retraining procedure employed. Extensive experiments
demonstrate that SAPA offers a general and principled strategy that significantly
enhances various types of poisoning attacks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NAACL</abbr></div>

        <!-- Entry bib key -->
        <div id="he2024data" class="col-sm-8">
        <!-- Title -->
        <div class="title">Data Poisoning for In-context Learning</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Han Xu, Yue Xing, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Hui Liu, Makoto Yamada, Jiliang Tang' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics</em>, Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2402.02160" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In the domain of large language models (LLMs), in-context 
      learning (ICL) has been recognized for its innovative ability to adapt to new tasks, 
      relying on examples rather than retraining or fine-tuning. This paper delves into 
      the critical issue of ICL’s susceptibility to data poisoning attacks, an area not 
      yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable 
      of manipulating example data to degrade model performance. To address this, 
      we introduce ICLPoison, a specialized attacking framework conceived to exploit 
      the learning mechanisms of ICL. Our approach uniquely employs discrete text 
      perturbations to strategically influence the hidden states of LLMs during the 
      ICL process. We outline three representative strategies to implement attacks 
      under our framework, each rigorously evaluated across a variety of models and 
      tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, 
      demonstrate that ICL’s performance is significantly compromised under our framework. 
      These revelations indicate an urgent need for enhanced defense mechanisms to safeguard 
      the integrity and reliability of LLMs in applications relying on in-context learning.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">Stat</abbr></div>

        <!-- Entry bib key -->
        <div id="he2024towards" class="col-sm-8">
        <!-- Title -->
        <div class="title">Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Yingqian Cui, Han Xu, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Hui Liu, Makoto Yamada, Jiliang Tang, Yue Xing' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Stat</em>, Apr 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.70045" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs’ behaviours in classification tasks.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="he2025multi" class="col-sm-8">
        <!-- Title -->
        <div class="title">Multi-Faceted Studies on Data Poisoning can Advance LLM Development</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Yue Xing, Han Xu, and
          <span class="more-authors" title="click to view 2 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '2 more authors' ? 'Zhen Xiang, Jiliang Tang' : '2 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">2 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2502.14182" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The lifecycle of large language models (LLMs)
is far more complex than that of traditional machine learning models, involving multiple training
stages, diverse data sources, and varied inference
methods. While prior research on data poisoning attacks has primarily focused on the safety
vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection,
rigorous data cleaning, and the multistage nature
of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior
as intended. Given these challenges, this position paper proposes rethinking the role of data
poisoning and argue that multi-faceted studies
on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate
and address real safety risks to LLMs. From a
trustworthiness perspective, data poisoning can be
leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs,
and hallucinations. Moreover, from a mechanism
perspective, data poisoning can provide valuable
insights into LLMs, particularly the interplay between data and model behavior, driving a deeper
understanding of their underlying mechanisms.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div>

        <!-- Entry bib key -->
        <div id="he2025red" class="col-sm-8">
        <!-- Title -->
        <div class="title">Red-Teaming LLM Multi-Agent Systems via Communication Attacks</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Yupin Lin, Shen Dong, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Han Xu, Yue Xing, Hui Liu' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2502.14847" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>tems (LLM-MAS) have revolutionized complex problem-solving capability by enabling
sophisticated agent collaboration through
message-based communications. While the
communication framework is crucial for agent
coordination, it also introduces a critical yet
unexplored security vulnerability. In this work,
we introduce Agent-in-the-Middle (AiTM), a
novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise
individual agents, AiTM demonstrates how an
adversary can compromise entire multi-agent
systems by only manipulating the messages
passing between agents. To enable the attack
under the challenges of limited control and rolerestricted communication format, we develop
an LLM-powered adversarial agent with a reflection mechanism that generates contextuallyaware malicious instructions. Our comprehensive evaluation across various frameworks,
communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures
in multi-agent systems.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div>

        <!-- Entry bib key -->
        <div id="wang2025unveiling" class="col-sm-8">
        <!-- Title -->
        <div class="title">Unveiling Privacy Risks in LLM Agent Memory</div>
        <!-- Author -->
        <div class="author">
        

        Bo Wang, Weiyi He, Shenglai Zeng, and
          <span class="more-authors" title="click to view 4 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '4 more authors' ? 'Zhen Xiang, Yue Xing, Jiliang Tang, Pengfei He' : '4 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">4 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In </em>, Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2502.13172" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent’s and the attacker’s perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="dong2025practical" class="col-sm-8">
        <!-- Title -->
        <div class="title">A practical memory injection attack against llm agents</div>
        <!-- Author -->
        <div class="author">
        

        Shen Dong, Shaochen Xu, <em>Pengfei He</em>, and
          <span class="more-authors" title="click to view 5 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '5 more authors' ? 'Yige Li, Jiliang Tang, Tianming Liu, Hui Liu, Zhen Xiang' : '5 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">5 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2503.03704" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Agents based on large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, that enables the injection of malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps leading to undesirable agent actions when executing the victim user’s query. Specifically, we introduce a sequence of bridging steps to link the victim query to the malicious reasoning steps. During the injection of the malicious record, we propose an indication prompt to guide the agent to autonomously generate our designed bridging steps. We also propose a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing the victim query comes after. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting practical risks of LLM agents.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">preprint</abbr></div>

        <!-- Entry bib key -->
        <div id="he2025comprehensivevulnerabilityanalysisnecessary" class="col-sm-8">
        <!-- Title -->
        <div class="title">Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS</div>
        <!-- Author -->
        <div class="author">
        

        <em>Pengfei He</em>, Yue Xing, Shen Dong, and
          <span class="more-authors" title="click to view 8 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '8 more authors' ? 'Juanhui Li, Zhenwei Dai, Xianfeng Tang, Hui Liu, Han Xu, Zhen Xiang, Charu C. Aggarwal, Hui Liu' : '8 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">8 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          Apr 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/pdf/2506.01245" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-arxiv-id="2506.01245"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper argues that a comprehensive vulnerability analysis is essential for building trustworthy Large Language Model-based Multi-Agent Systems (LLM-MAS). These systems, which consist of multiple LLM-powered agents working collaboratively, are increasingly deployed in high-stakes applications but face novel security threats due to their complex structures. While single-agent vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces through inter-agent communication, trust relationships, and tool integration that remain significantly underexplored. We present a systematic framework for vulnerability analysis of LLM-MAS that unifies diverse research. For each type of vulnerability, we define formal threat models grounded in practical attacker capabilities and illustrate them using real-world LLM-MAS applications. This formulation enables rigorous quantification of vulnerability across different architectures and provides a foundation for designing meaningful evaluation benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to compositional effects – vulnerabilities in individual components can cascade through agent communication, creating threat models not present in single-agent systems. We conclude by identifying critical open challenges: (1) developing benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2) considering new potential attacks specific to multi-agent architectures, and (3) implementing trust management systems that can enforce security in LLM-MAS. This research provides essential groundwork for future efforts to enhance LLM-MAS trustworthiness as these systems continue their expansion into critical applications.</p>
          </div>
        </div>
      </div>
</li>
</ol>
          </div>


          <!-- Social -->
            <div class="social">
              <div class="contact-icons">
                <a href="mailto:%68%65%70%65%6E%67%66%31@%6D%73%75.%65%64%75" title="email"><i class="fas fa-envelope"></i></a>
            <a href="https://scholar.google.com/citations?user=nsSrd6kAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a>
            <a href="https://github.com/PengfeiHePower" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a>
            <a href="https://www.linkedin.com/in/pengfei-he-0a9185169" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a>
            <a href="https://twitter.com/PengfeiHePower" title="Twitter" rel="external nofollow noopener" target="_blank"><i class="fab fa-twitter"></i></a>
            

              </div>

              <div class="contact-note">
                Please follow my X account and contact me via emails.

              </div>

            </div>
        </article>

</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2025 Pengfei  He. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
