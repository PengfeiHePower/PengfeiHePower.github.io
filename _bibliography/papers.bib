---
---

@string{aps = {ieee,}}



@inproceedings{10.1145/3511808.3557130,
abbr = {CIKM},
author = {He, Pengfei and Liu, Haochen and Zhao, Xiangyu and Liu, Hui and Tang, Jiliang},
title = {PROPN: Personalized Probabilistic Strategic Parameter Optimization in Recommendations},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557130},
doi = {10.1145/3511808.3557130},
booktitle = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management (CIKM)},
pages = {3152–3161},
numpages = {10},
keywords = {recommender system, demographic information, reinforcement learning, strategic parameter optimizing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{JMLR:v24:21-1254,
  abbr = {JMLR},
  author  = {Nicolas Garcia Trillos and Pengfei He and Chenghui Li},
  title   = {Large sample spectral analysis of graph-based multi-manifold clustering},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2023},
  volume  = {24},
  number  = {143},
  pages   = {1--71},
  url     = {http://jmlr.org/papers/v24/21-1254.html},
  abstract = {In this work we study statistical properties of graph-based algorithms for multi-manifold
clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a
given Euclidean data set when this one is assumed to be obtained by sampling a distribution
on a union of manifolds M = M1 ∪ · · · ∪ MN that may intersect with each other and
that may have different dimensions. We investigate sufficient conditions that similarity
graphs on data sets must satisfy in order for their corresponding graph Laplacians to
capture the right geometric information to solve the MMC problem. Precisely, we provide
high probability error bounds for the spectral approximation of a tensorized Laplacian on
M with a suitable graph Laplacian built from the observations; the recovered tensorized
Laplacian contains all geometric information of all the individual underlying manifolds. We
provide an example of a family of similarity graphs, which we call annular proximity graphs
with angle constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of tangent planes.
Extensive numerical experiments expand the insights that our theory provides on the MMC
problem.},
  pdf = {http://jmlr.org/papers/v24/21-1254.html}
}

@inproceedings{xu2023probabilistic,
  abbr = {ICML},
  title={Probabilistic Categorical Adversarial Attack and Adversarial Training},
  author={Xu, Han and He, Pengfei and Ren, Jie and Wan, Yuxuan and Liu, Zitao and Liu, Hui and Tang, Jiliang},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={38428--38442},
  year={2023},
  abstract={The studies on adversarial attacks and defenses
have greatly improved the robustness of Deep
Neural Networks (DNNs). Most advanced approaches have been overwhelmingly designed for
continuous data such as images. However, these
achievements are still hard to be generalized to
categorical data. To bridge this gap, we propose
a novel framework, Probabilistic Categorical Adversarial Attack (or PCAA). It transfers the discrete optimization problem of finding categorical
adversarial examples to a continuous problem that
can be solved via gradient-based methods. We analyze the optimality (attack success rate) and time
complexity of PCAA to demonstrate its significant advantage over current search-based attacks.
More importantly, through extensive empirical
studies, we demonstrate that the well-established
defenses for continuous data, such as adversarial
training and TRADES, can be easily accommodated to defend DNNs for categorical data},
  organization={PMLR},
  pdf = {https://proceedings.mlr.press/v202/xu23e.html}
}
@inproceedings{he2023sharpness,
  abbr = {ICLR Spotlight},
  title={Sharpness-Aware Data Poisoning Attack},
  author={He, Pengfei and Xu, Han and Ren, Jie and Cui, Yingqian and Liu, Hui and Aggarwal, Charu C and Tang, Jiliang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  note={Spotlight Paper, 5%},
  abstract={Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs)
against data poisoning attacks. These attacks aim to inject poisoning samples into
the models’ training dataset such that the trained models have inference failures.
While previous studies have executed different types of attacks, one major challenge
that greatly limits their effectiveness is the uncertainty of the re-training process
after the injection of poisoning samples, including the re-training initialization or
algorithms. To address this challenge, we propose a novel attack method called
“Sharpness-Aware Data Poisoning Attack (SAPA)”. In particular, it leverages the
concept of DNNs’ loss landscape sharpness to optimize the poisoning effect on the
worst re-trained model. It helps enhance the preservation of the poisoning effect,
regardless of the specific retraining procedure employed. Extensive experiments
demonstrate that SAPA offers a general and principled strategy that significantly
enhances various types of poisoning attacks.},
  pdf = {/assets/pdf/Sharpness_aware_poisoning_attacks.pdf}
}

@article{10.1145/3715073.3715079,
author = {Cui, Yingqian and Ren, Jie and Xu, Han and He, Pengfei and Liu, Hui and Sun, Lichao and Xing, Yue and Tang, Jiliang},
title = {DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models},
year = {2025},
abbr={SIGKDD Explor. Newsl.},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3715073.3715079},
doi = {10.1145/3715073.3715079},
abstract = {Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {60–75},
numpages = {16},
pdf={https://dl.acm.org/doi/10.1145/3715073.3715079}
}




@inproceedings{zeng-etal-2024-good,
    abbr = "ACL",
    title = "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation ({RAG})",
    author = "Zeng, Shenglai  and
      Zhang, Jiankun  and
      He, Pengfei  and
      Liu, Yiding  and
      Xing, Yue  and
      Xu, Han  and
      Ren, Jie  and
      Chang, Yi  and
      Wang, Shuaiqiang  and
      Yin, Dawei  and
      Tang, Jiliang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.267",
    doi = "10.18653/v1/2024.findings-acl.267",
    pages = "4505--4524",
    abstract = "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs{'} training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.",
    pdf={https://aclanthology.org/2024.findings-acl.267/}
}

@article{hestealthy,
  title={Stealthy Backdoor Attack via Confidence-driven Sampling},
  abbr={TMLR},
  author={He, Pengfei and Xing, Yue and Xu, Han and Ren, Jie and Cui, Yingqian and Zeng, Shenglai and Tang, Jiliang and Yamada, Makoto and Sabokrou, Mohammad},
  journal={Transactions on Machine Learning Research},
  pdf={https://openreview.net/pdf?id=Flh5EXz8dA},
  abstract={Backdoor attacks facilitate unauthorized control in the testing stage by carefully injecting
harmful triggers during the training phase of deep neural networks. Previous works have
focused on improving the stealthiness of the trigger while randomly selecting samples to
attack. However, we find that random selection harms the stealthiness of the model. In
this paper, we identify significant pitfalls of random sampling, which make the attacks more
detectable and easier to defend against. To improve the stealthiness of existing attacks, we
introduce a method of strategically poisoning samples near the model’s decision boundary,
aiming to minimally alter the model’s behavior (decision boundary) before and after backdooring. Our main insight for detecting boundary samples is exploiting the confidence scores
as a metric for being near the decision boundary and selecting those to poison (inject) the
attack. The proposed approach makes it significantly harder for defenders to identify the
attacks. Our method is versatile and independent of any specific trigger design. We provide
theoretical insights and conduct extensive experiments to demonstrate the effectiveness of
the proposed method.}
}

@article{10.1145/3715073.3715080,
author = {Cui, Yingqian and Ren, Jie and Lin, Yuping and Xu, Han and He, Pengfei and Xing, Yue and Lyu, Lingjuan and Fan, Wenqi and Liu, Hui and Tang, Jiliang},
abbr={SIGKDD Explor. Newsl.},
title = {FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3715073.3715080},
doi = {10.1145/3715073.3715080},
abstract = {Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {76–88},
numpages = {13},
pdf={https://dl.acm.org/doi/10.1145/3715073.3715080}
}




@inproceedings{zeng-etal-2024-exploring,
    abbr = "ACL",
    title = "Exploring Memorization in Fine-tuned Language Models",
    author = "Zeng, Shenglai  and
      Li, Yaxin  and
      Ren, Jie  and
      Liu, Yiding  and
      Xu, Han  and
      He, Pengfei  and
      Xing, Yue  and
      Wang, Shuaiqiang  and
      Tang, Jiliang  and
      Yin, Dawei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.216",
    doi = "10.18653/v1/2024.acl-long.216",
    pages = "3917--3948",
    abstract = "Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models{'} (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution.",
    pdf="https://aclanthology.org/2024.acl-long.216.pdf"
}

@inproceedings{xu2023generalization,
    abbr = {EMNLP},
    title = "On the Generalization of Training-based ChatGPT Detection Methods",
    author = "Han Xu and Jie Ren and Pengfei He and Shenglai Zeng and Yingqian Cui and Amy Liu and Hui Liu and Jiliang Tang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data collection strategies for ChatGPT detection.",
    pdf="https://aclanthology.org/2024.findings-emnlp.424.pdf"
}

@inproceedings{lin2024towards,
    abbr = {EMNLP},
    title = "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis",
    author = "He, Pengfei and Lin, Yuping and Xu, Han and Xing, Yue and Yamada, Makoto and Liu, Hui and Tang, Jiliang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.",
    pdf="https://aclanthology.org/2024.emnlp-main.401/"
}


@inproceedings{he2024data,
      title={Data Poisoning for In-context Learning}, 
      abbr={NAACL},
      author={Pengfei He and Han Xu and Yue Xing and Hui Liu and Makoto Yamada and Jiliang Tang},
      month = {Apr},
      year={2025},
      booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics},
      abstract={In the domain of large language models (LLMs), in-context 
      learning (ICL) has been recognized for its innovative ability to adapt to new tasks, 
      relying on examples rather than retraining or fine-tuning. This paper delves into 
      the critical issue of ICL's susceptibility to data poisoning attacks, an area not 
      yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable 
      of manipulating example data to degrade model performance. To address this, 
      we introduce ICLPoison, a specialized attacking framework conceived to exploit 
      the learning mechanisms of ICL. Our approach uniquely employs discrete text 
      perturbations to strategically influence the hidden states of LLMs during the 
      ICL process. We outline three representative strategies to implement attacks 
      under our framework, each rigorously evaluated across a variety of models and 
      tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, 
      demonstrate that ICL's performance is significantly compromised under our framework. 
      These revelations indicate an urgent need for enhanced defense mechanisms to safeguard 
      the integrity and reliability of LLMs in applications relying on in-context learning.},
      pdf={https://arxiv.org/abs/2402.02160}
}

@misc{ren2024copyright,
      title={Copyright Protection in Generative AI: A Technical Perspective}, 
      abbr={preprint},
      author={Jie Ren and Han Xu and Pengfei He and Yingqian Cui and Shenglai Zeng and Jiankun Zhang and Hongzhi Wen and Jiayuan Ding and Hui Liu and Yi Chang and Jiliang Tang},
      year={2024},
      eprint={2402.02333},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      pdf={https://arxiv.org/pdf/2402.02333},
      abstract={Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.}
}

@inproceedings{cui2024superiority,
      title={Superiority of Multi-Head Attention in In-Context Linear Regression}, 
      abbr={AISTATS},
      author={Yingqian Cui and Jie Ren and Pengfei He and Jiliang Tang and Yue Xing},
      year={2025},
      abstract={We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head attention in the transformer architecture.},
      pdf={https://arxiv.org/pdf/2401.17426}
}

@article{he2024towards,
  abbr = {Stat},
  title={Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study},
  author={He, Pengfei and Cui, Yingqian and Xu, Han and Liu, Hui and Yamada, Makoto and Tang, Jiliang and Xing, Yue},
  journal={Stat},
  year={2024},
  pdf={https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.70045},
  abstract={In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviours in classification tasks.}
}

@inproceedings{Cui2024ATU,
      title={A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration}, 
      abbr={AISTSTS},
      author={Yingqian Cui and Pengfei He and Xianfeng Tang and Qi He and Chen Luo and Jiliang Tang and Yue Xing},
      year={2025},
      abstract={Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.},
      pdf={https://arxiv.org/pdf/2410.16540}
}

@misc{he2024makellmsbetterzeroshot,
      title={Make LLMs better zero-shot reasoners: Structure-orientated autonomous reasoning}, 
      abbr={preprint},
      author={Pengfei He and Zitao Li and Yue Xing and Yaling Li and Jiliang Tang and Bolin Ding},
      year={2024},
      eprint={2410.19000},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19000},
      pdf={https://arxiv.org/abs/2410.19000},
      abstract={Zero-shot reasoning methods with Large Language Models (LLMs) offer significant advantages including great generalization to novel tasks and reduced dependency on human-crafted examples. However, the current zero-shot methods still have limitations in complex tasks, e.g., answering questions that require multi-step reasoning. In this paper, we address this limitation by introducing a novel structure-oriented analysis method to help LLMs better understand the question and guide the problem-solving process of LLMs. We first demonstrate how the existing reasoning strategies, Chain-of-Thought and ReAct, can benefit from our structure-oriented analysis. In addition to empirical investigations, we leverage the probabilistic graphical model to theoretically explain why our structure-oriented analysis can improve the LLM reasoning process. To further improve the reliability in complex question-answering tasks, we propose a multi-agent reasoning system, Structure-oriented Autonomous Reasoning Agents (SARA), that can better enforce the reasoning process following our structure-oriented analysis by refinement techniques and is equipped with external knowledge retrieval capability to reduce factual errors. Extensive experiments verify the effectiveness of the proposed reasoning system. Surprisingly, in some cases, the system even surpasses few-shot methods. Finally, the system not only improves reasoning accuracy in complex tasks but also demonstrates robustness against potential attacks that corrupt the reasoning process.}
}
