---
---

@string{aps = {ieee,}}



@inproceedings{10.1145/3511808.3557130,
abbr = {CIKM},
author = {He, Pengfei and Liu, Haochen and Zhao, Xiangyu and Liu, Hui and Tang, Jiliang},
title = {PROPN: Personalized Probabilistic Strategic Parameter Optimization in Recommendations},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557130},
doi = {10.1145/3511808.3557130},
booktitle = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management (CIKM)},
pages = {3152–3161},
numpages = {10},
keywords = {recommender system, demographic information, reinforcement learning, strategic parameter optimizing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{JMLR:v24:21-1254,
  abbr = {JMLR},
  author  = {Nicolas Garcia Trillos* and Pengfei He* and Chenghui Li*},
  title   = {Large sample spectral analysis of graph-based multi-manifold clustering},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2023},
  volume  = {24},
  number  = {143},
  pages   = {1--71},
  url     = {http://jmlr.org/papers/v24/21-1254.html},
  abstract = {In this work we study statistical properties of graph-based algorithms for multi-manifold
clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a
given Euclidean data set when this one is assumed to be obtained by sampling a distribution
on a union of manifolds M = M1 ∪ · · · ∪ MN that may intersect with each other and
that may have different dimensions. We investigate sufficient conditions that similarity
graphs on data sets must satisfy in order for their corresponding graph Laplacians to
capture the right geometric information to solve the MMC problem. Precisely, we provide
high probability error bounds for the spectral approximation of a tensorized Laplacian on
M with a suitable graph Laplacian built from the observations; the recovered tensorized
Laplacian contains all geometric information of all the individual underlying manifolds. We
provide an example of a family of similarity graphs, which we call annular proximity graphs
with angle constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of tangent planes.
Extensive numerical experiments expand the insights that our theory provides on the MMC
problem.},
  pdf = {http://jmlr.org/papers/v24/21-1254.html},
  selected={true}
}

@inproceedings{xu2023probabilistic,
  abbr = {ICML},
  title={Probabilistic Categorical Adversarial Attack and Adversarial Training},
  author={Xu, Han and He, Pengfei and Ren, Jie and Wan, Yuxuan and Liu, Zitao and Liu, Hui and Tang, Jiliang},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={38428--38442},
  year={2023},
  abstract={The studies on adversarial attacks and defenses
have greatly improved the robustness of Deep
Neural Networks (DNNs). Most advanced approaches have been overwhelmingly designed for
continuous data such as images. However, these
achievements are still hard to be generalized to
categorical data. To bridge this gap, we propose
a novel framework, Probabilistic Categorical Adversarial Attack (or PCAA). It transfers the discrete optimization problem of finding categorical
adversarial examples to a continuous problem that
can be solved via gradient-based methods. We analyze the optimality (attack success rate) and time
complexity of PCAA to demonstrate its significant advantage over current search-based attacks.
More importantly, through extensive empirical
studies, we demonstrate that the well-established
defenses for continuous data, such as adversarial
training and TRADES, can be easily accommodated to defend DNNs for categorical data},
  organization={PMLR},
  pdf = {https://proceedings.mlr.press/v202/xu23e.html}
}
@inproceedings{he2023sharpness,
  abbr = {ICLR Spotlight},
  title={Sharpness-Aware Data Poisoning Attack},
  author={He, Pengfei and Xu, Han and Ren, Jie and Cui, Yingqian and Liu, Hui and Aggarwal, Charu C and Tang, Jiliang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  note={Spotlight Paper, 5%},
  abstract={Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs)
against data poisoning attacks. These attacks aim to inject poisoning samples into
the models’ training dataset such that the trained models have inference failures.
While previous studies have executed different types of attacks, one major challenge
that greatly limits their effectiveness is the uncertainty of the re-training process
after the injection of poisoning samples, including the re-training initialization or
algorithms. To address this challenge, we propose a novel attack method called
“Sharpness-Aware Data Poisoning Attack (SAPA)”. In particular, it leverages the
concept of DNNs’ loss landscape sharpness to optimize the poisoning effect on the
worst re-trained model. It helps enhance the preservation of the poisoning effect,
regardless of the specific retraining procedure employed. Extensive experiments
demonstrate that SAPA offers a general and principled strategy that significantly
enhances various types of poisoning attacks.},
  pdf = {https://arxiv.org/pdf/2305.14851},
  selected={true}
}

@article{10.1145/3715073.3715079,
author = {Cui, Yingqian and Ren, Jie and Xu, Han and He, Pengfei and Liu, Hui and Sun, Lichao and Xing, Yue and Tang, Jiliang},
title = {DiffusionShield: A Watermark for Data Copyright Protection against Generative Diffusion Models},
year = {2025},
abbr={SIGKDD Explor.},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3715073.3715079},
doi = {10.1145/3715073.3715079},
abstract = {Recently, Generative Diffusion Models (GDMs) have shown remarkable abilities in learning and generating images, fostering a large community of GDMs. However, the unrestricted proliferation has raised serious concerns on copyright issues. For example, artists become concerned that GDMs could effortlessly replicate their unique artworks without permission. In response to these challenges, we introduce a novel watermark scheme, Diffusion Shield, against GDMs. It protects images from infringement by encoding the ownership message into an imperceptible watermark and injecting it into images. This watermark can be easily learned by GDMs and will be reproduced in generated images. By detecting the watermark in generated images, the infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, Diffusion Shield ensures low distortion of the original image, high watermark detection performance, and lengthy encoded messages. We conduct rigorous and comprehensive experiments to show its effectiveness in defending against infringement by GDMs and its superiority over traditional watermark methods.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {60–75},
numpages = {16},
pdf={https://dl.acm.org/doi/10.1145/3715073.3715079}
}




@inproceedings{zeng-etal-2024-good,
    abbr = "ACL",
    title = "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation ({RAG})",
    author = "Zeng, Shenglai  and
      Zhang, Jiankun  and
      He, Pengfei  and
      Liu, Yiding  and
      Xing, Yue  and
      Xu, Han  and
      Ren, Jie  and
      Chang, Yi  and
      Wang, Shuaiqiang  and
      Yin, Dawei  and
      Tang, Jiliang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.267",
    doi = "10.18653/v1/2024.findings-acl.267",
    pages = "4505--4524",
    abstract = "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs{'} training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.",
    pdf={https://aclanthology.org/2024.findings-acl.267/}
}

@article{hestealthy,
  title={Stealthy Backdoor Attack via Confidence-driven Sampling},
  abbr={TMLR},
  year={2024},
  author={He, Pengfei and Xing, Yue and Xu, Han and Ren, Jie and Cui, Yingqian and Zeng, Shenglai and Tang, Jiliang and Yamada, Makoto and Sabokrou, Mohammad},
  journal={Transactions on Machine Learning Research},
  pdf={https://openreview.net/pdf?id=Flh5EXz8dA},
  abstract={Backdoor attacks facilitate unauthorized control in the testing stage by carefully injecting
harmful triggers during the training phase of deep neural networks. Previous works have
focused on improving the stealthiness of the trigger while randomly selecting samples to
attack. However, we find that random selection harms the stealthiness of the model. In
this paper, we identify significant pitfalls of random sampling, which make the attacks more
detectable and easier to defend against. To improve the stealthiness of existing attacks, we
introduce a method of strategically poisoning samples near the model’s decision boundary,
aiming to minimally alter the model’s behavior (decision boundary) before and after backdooring. Our main insight for detecting boundary samples is exploiting the confidence scores
as a metric for being near the decision boundary and selecting those to poison (inject) the
attack. The proposed approach makes it significantly harder for defenders to identify the
attacks. Our method is versatile and independent of any specific trigger design. We provide
theoretical insights and conduct extensive experiments to demonstrate the effectiveness of
the proposed method.}
}

@article{10.1145/3715073.3715080,
author = {Cui, Yingqian and Ren, Jie and Lin, Yuping and Xu, Han and He, Pengfei and Xing, Yue and Lyu, Lingjuan and Fan, Wenqi and Liu, Hui and Tang, Jiliang},
abbr={SIGKDD Explor.},
title = {FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models},
year = {2025},
issue_date = {December 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {2},
issn = {1931-0145},
url = {https://doi.org/10.1145/3715073.3715080},
doi = {10.1145/3715073.3715080},
abstract = {Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.},
journal = {SIGKDD Explor. Newsl.},
month = jan,
pages = {76–88},
numpages = {13},
pdf={https://dl.acm.org/doi/10.1145/3715073.3715080}
}




@inproceedings{zeng-etal-2024-exploring,
    abbr = "ACL",
    title = "Exploring Memorization in Fine-tuned Language Models",
    author = "Zeng, Shenglai  and
      Li, Yaxin  and
      Ren, Jie  and
      Liu, Yiding  and
      Xu, Han  and
      He, Pengfei  and
      Xing, Yue  and
      Wang, Shuaiqiang  and
      Tang, Jiliang  and
      Yin, Dawei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.216",
    doi = "10.18653/v1/2024.acl-long.216",
    pages = "3917--3948",
    abstract = "Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models{'} (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution.",
    pdf="https://aclanthology.org/2024.acl-long.216.pdf"
}

@inproceedings{xu2023generalization,
    abbr = {EMNLP},
    title = "On the Generalization of Training-based ChatGPT Detection Methods",
    author = "Han Xu and Jie Ren and Pengfei He and Shenglai Zeng and Yingqian Cui and Amy Liu and Hui Liu and Jiliang Tang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data collection strategies for ChatGPT detection.",
    pdf="https://aclanthology.org/2024.findings-emnlp.424.pdf"
}

@inproceedings{lin2024towards,
    abbr = {EMNLP},
    title = "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis",
    author = "Lin, Yuping* and He, Pengfei* and Xu, Han and Xing, Yue and Yamada, Makoto and Liu, Hui and Tang, Jiliang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.",
    pdf="https://aclanthology.org/2024.emnlp-main.401/"
}


@inproceedings{he2024data,
      title={Data Poisoning for In-context Learning}, 
      abbr={NAACL},
      author={Pengfei He and Han Xu and Yue Xing and Hui Liu and Makoto Yamada and Jiliang Tang},
      month = {Apr},
      year={2025},
      booktitle={Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics},
      abstract={In the domain of large language models (LLMs), in-context 
      learning (ICL) has been recognized for its innovative ability to adapt to new tasks, 
      relying on examples rather than retraining or fine-tuning. This paper delves into 
      the critical issue of ICL's susceptibility to data poisoning attacks, an area not 
      yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable 
      of manipulating example data to degrade model performance. To address this, 
      we introduce ICLPoison, a specialized attacking framework conceived to exploit 
      the learning mechanisms of ICL. Our approach uniquely employs discrete text 
      perturbations to strategically influence the hidden states of LLMs during the 
      ICL process. We outline three representative strategies to implement attacks 
      under our framework, each rigorously evaluated across a variety of models and 
      tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, 
      demonstrate that ICL's performance is significantly compromised under our framework. 
      These revelations indicate an urgent need for enhanced defense mechanisms to safeguard 
      the integrity and reliability of LLMs in applications relying on in-context learning.},
      pdf={https://arxiv.org/abs/2402.02160},
      selected={true}
}

@misc{ren2024copyright,
      title={Copyright Protection in Generative AI: A Technical Perspective}, 
      abbr={preprint},
      author={Jie Ren and Han Xu and Pengfei He and Yingqian Cui and Shenglai Zeng and Jiankun Zhang and Hongzhi Wen and Jiayuan Ding and Hui Liu and Yi Chang and Jiliang Tang},
      year={2024},
      eprint={2402.02333},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      pdf={https://arxiv.org/pdf/2402.02333},
      abstract={Generative AI has witnessed rapid advancement in recent years, expanding their capabilities to create synthesized content such as text, images, audio, and code. The high fidelity and authenticity of contents generated by these Deep Generative Models (DGMs) have sparked significant copyright concerns. There have been various legal debates on how to effectively safeguard copyrights in DGMs. This work delves into this issue by providing a comprehensive overview of copyright protection from a technical perspective. We examine from two distinct viewpoints: the copyrights pertaining to the source data held by the data owners and those of the generative models maintained by the model builders. For data copyright, we delve into methods data owners can protect their content and DGMs can be utilized without infringing upon these rights. For model copyright, our discussion extends to strategies for preventing model theft and identifying outputs generated by specific models. Finally, we highlight the limitations of existing techniques and identify areas that remain unexplored. Furthermore, we discuss prospective directions for the future of copyright protection, underscoring its importance for the sustainable and ethical development of Generative AI.}
}

@inproceedings{cui2024superiority,
      title={Superiority of Multi-Head Attention in In-Context Linear Regression}, 
      abbr={AISTATS},
      author={Yingqian Cui and Jie Ren and Pengfei He and Jiliang Tang and Yue Xing},
      year={2025},
      abstract={We present a theoretical analysis of the performance of transformer with softmax attention in in-context learning with linear regression tasks. While the existing literature predominantly focuses on the convergence of transformers with single-/multi-head attention, our research centers on comparing their performance. We conduct an exact theoretical analysis to demonstrate that multi-head attention with a substantial embedding dimension performs better than single-head attention. When the number of in-context examples D increases, the prediction loss using single-/multi-head attention is in O(1/D), and the one for multi-head attention has a smaller multiplicative constant. In addition to the simplest data distribution setting, we consider more scenarios, e.g., noisy labels, local examples, correlated features, and prior knowledge. We observe that, in general, multi-head attention is preferred over single-head attention. Our results verify the effectiveness of the design of multi-head attention in the transformer architecture.},
      pdf={https://arxiv.org/pdf/2401.17426}
}

@article{he2024towards,
  abbr = {Stat},
  title={Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study},
  author={He, Pengfei and Cui, Yingqian and Xu, Han and Liu, Hui and Yamada, Makoto and Tang, Jiliang and Xing, Yue},
  journal={Stat},
  year={2024},
  pdf={https://onlinelibrary.wiley.com/doi/full/10.1002/sta4.70045},
  abstract={In-context learning (ICL) has emerged as a powerful capability for large language models (LLMs) to adapt to downstream tasks by leveraging a few (demonstration) examples. Despite its effectiveness, the mechanism behind ICL remains underexplored. To better understand how ICL integrates the examples with the knowledge learned by the LLM during pre-training (i.e., pre-training knowledge) and how the examples impact ICL, this paper conducts a theoretical study in binary classification tasks. In particular, we introduce a probabilistic model extending from the Gaussian mixture model to exactly quantify the impact of pre-training knowledge, label frequency and label noise on the prediction accuracy. Based on our analysis, when the pre-training knowledge contradicts the knowledge in the examples, whether ICL prediction relies more on the pre-training knowledge or the examples depends on the number of examples. In addition, the label frequency and label noise of the examples both affect the accuracy of the ICL prediction, where the minor class has a lower accuracy, and how the label noise impacts the accuracy is determined by the specific noise level of the two classes. Extensive simulations are conducted to verify the correctness of the theoretical results, and real-data experiments also align with the theoretical insights. Our work reveals the role of pre-training knowledge and examples in ICL, offering a deeper understanding of LLMs' behaviours in classification tasks.},
  selected={true}
}

@inproceedings{Cui2024ATU,
      title={A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and Error-Aware Demonstration}, 
      abbr={AISTSTS},
      author={Yingqian Cui and Pengfei He and Xianfeng Tang and Qi He and Chen Luo and Jiliang Tang and Yue Xing},
      year={2025},
      abstract={Few-shot Chain-of-Thought (CoT) prompting has demonstrated strong performance in improving the reasoning capabilities of large language models (LLMs). While theoretical investigations have been conducted to understand CoT, the underlying transformer used in these studies isolates the CoT reasoning process into separated in-context learning steps (Stepwise ICL). In this work, we theoretically show that, compared to Stepwise ICL, the transformer gains better error correction ability and more accurate predictions if the reasoning from earlier steps (Coherent CoT) is integrated. Given that this coherent reasoning changes the behavior of the transformer, we further investigate the sensitivity of the transformer with Coherent CoT when the demonstration examples are corrupted at the inference stage. Our theoretical results indicate that the transformer is more sensitive to errors in intermediate reasoning steps than the final outcome. Building upon this observation, we propose an improvement on CoT by incorporating both correct and incorrect reasoning paths in the demonstration. Our experiments validate the effectiveness of the proposed approach.},
      pdf={https://arxiv.org/pdf/2410.16540}
}

@misc{he2024makellmsbetterzeroshot,
      title={Make LLMs better zero-shot reasoners: Structure-orientated autonomous reasoning}, 
      abbr={preprint},
      author={Pengfei He and Zitao Li and Yue Xing and Yaling Li and Jiliang Tang and Bolin Ding},
      year={2024},
      eprint={2410.19000},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2410.19000},
      pdf={https://arxiv.org/abs/2410.19000},
      abstract={Zero-shot reasoning methods with Large Language Models (LLMs) offer significant advantages including great generalization to novel tasks and reduced dependency on human-crafted examples. However, the current zero-shot methods still have limitations in complex tasks, e.g., answering questions that require multi-step reasoning. In this paper, we address this limitation by introducing a novel structure-oriented analysis method to help LLMs better understand the question and guide the problem-solving process of LLMs. We first demonstrate how the existing reasoning strategies, Chain-of-Thought and ReAct, can benefit from our structure-oriented analysis. In addition to empirical investigations, we leverage the probabilistic graphical model to theoretically explain why our structure-oriented analysis can improve the LLM reasoning process. To further improve the reliability in complex question-answering tasks, we propose a multi-agent reasoning system, Structure-oriented Autonomous Reasoning Agents (SARA), that can better enforce the reasoning process following our structure-oriented analysis by refinement techniques and is equipped with external knowledge retrieval capability to reduce factual errors. Extensive experiments verify the effectiveness of the proposed reasoning system. Surprisingly, in some cases, the system even surpasses few-shot methods. Finally, the system not only improves reasoning accuracy in complex tasks but also demonstrates robustness against potential attacks that corrupt the reasoning process.}
}

@misc{zeng2024mitigating,
  title={Mitigating the privacy issues in retrieval-augmented generation (rag) via pure synthetic data},
  author={Zeng, Shenglai and Zhang, Jiankun and He, Pengfei and Ren, Jie and Zheng, Tianqi and Lu, Hanqing and Xu, Han and Liu, Hui and Xing, Yue and Tang, Jiliang},
  abbr={preprint},
  journal={arXiv preprint arXiv:2406.14773},
  year={2024},
  pdf={https://arxiv.org/pdf/2406.14773},
  abstract={Retrieval-augmented generation (RAG) enhances the outputs of language models by integrating relevant information retrieved from external knowledge sources. However, when the retrieval process involves private data, RAG systems may face severe privacy risks, potentially leading to the leakage of sensitive information. To address this issue, we propose using synthetic data as a privacy-preserving alternative for the retrieval data. We propose SAGE, a novel two-stage synthetic data generation paradigm. In the stage-1, we employ an attribute-based extraction and generation approach to preserve key contextual information from the original data. In the stage-2, we further enhance the privacy properties of the synthetic data through an agent-based iterative refinement process. Extensive experiments demonstrate that using our synthetic data as the retrieval context achieves comparable performance to using the original data while substantially reducing privacy risks. Our work takes the first step towards investigating the possibility of generating high-utility and privacy-preserving synthetic data for RAG, opening up new opportunities for the safe application of RAG systems in various domains.}
}

@misc{he2025multi,
  title={Multi-Faceted Studies on Data Poisoning can Advance LLM Development},
  author={He, Pengfei and Xing, Yue and Xu, Han and Xiang, Zhen and Tang, Jiliang},
  abbr={preprint},
  journal={arXiv preprint arXiv:2502.14182},
  year={2025},
  pdf={https://arxiv.org/pdf/2502.14182},
  abstract={The lifecycle of large language models (LLMs)
is far more complex than that of traditional machine learning models, involving multiple training
stages, diverse data sources, and varied inference
methods. While prior research on data poisoning attacks has primarily focused on the safety
vulnerabilities of LLMs, these attacks face significant challenges in practice. Secure data collection,
rigorous data cleaning, and the multistage nature
of LLM training make it difficult to inject poisoned data or reliably influence LLM behavior
as intended. Given these challenges, this position paper proposes rethinking the role of data
poisoning and argue that multi-faceted studies
on data poisoning can advance LLM development. From a threat perspective, practical strategies for data poisoning attacks can help evaluate
and address real safety risks to LLMs. From a
trustworthiness perspective, data poisoning can be
leveraged to build more robust LLMs by uncovering and mitigating hidden biases, harmful outputs,
and hallucinations. Moreover, from a mechanism
perspective, data poisoning can provide valuable
insights into LLMs, particularly the interplay between data and model behavior, driving a deeper
understanding of their underlying mechanisms.}
}

@inproceedings{he2025red,
abbr = "ACL",
  title={Red-Teaming LLM Multi-Agent Systems via Communication Attacks},
  author={He, Pengfei and Lin, Yupin and Dong, Shen and Xu, Han and Xing, Yue and Liu, Hui},
  abbr={ACL},
  journal={Association for Computational Linguistics},
  year={2025},
  abstract={tems (LLM-MAS) have revolutionized complex problem-solving capability by enabling
sophisticated agent collaboration through
message-based communications. While the
communication framework is crucial for agent
coordination, it also introduces a critical yet
unexplored security vulnerability. In this work,
we introduce Agent-in-the-Middle (AiTM), a
novel attack that exploits the fundamental communication mechanisms in LLM-MAS by intercepting and manipulating inter-agent messages. Unlike existing attacks that compromise
individual agents, AiTM demonstrates how an
adversary can compromise entire multi-agent
systems by only manipulating the messages
passing between agents. To enable the attack
under the challenges of limited control and rolerestricted communication format, we develop
an LLM-powered adversarial agent with a reflection mechanism that generates contextuallyaware malicious instructions. Our comprehensive evaluation across various frameworks,
communication structures, and real-world applications demonstrates that LLM-MAS is vulnerable to communication-based attacks, highlighting the need for robust security measures
in multi-agent systems.},
pdf={https://arxiv.org/pdf/2502.14847}
}

@inproceedings{wang2025unveiling,
  title={Unveiling Privacy Risks in LLM Agent Memory},
  author={Wang, Bo and He, Weiyi and He, Pengfei and Zeng, Shenglai and Xiang, Zhen and Xing, Yue and Tang, Jiliang},
  abbr={ACL},
  journal={Association for Computational Linguistics},
  year={2025},
  abstract={Large Language Model (LLM) agents have become increasingly prevalent across various real-world applications. They enhance decision-making by storing private user-agent interactions in the memory module for demonstrations, introducing new privacy risks for LLM agents. In this work, we systematically investigate the vulnerability of LLM agents to our proposed Memory EXTRaction Attack (MEXTRA) under a black-box setting. To extract private information from memory, we propose an effective attacking prompt design and an automated prompt generation method based on different levels of knowledge about the LLM agent. Experiments on two representative agents demonstrate the effectiveness of MEXTRA. Moreover, we explore key factors influencing memory leakage from both the agent's and the attacker's perspectives. Our findings highlight the urgent need for effective memory safeguards in LLM agent design and deployment.},
  pdf={https://arxiv.org/pdf/2502.13172}
}

@inproceedings{cui2025stepwise,
  title={Stepwise Perplexity-Guided Refinement for Efficient Chain-of-Thought Reasoning in Large Language Models},
  author={Cui, Yingqian and He, Pengfei and Zeng, Jingying and Liu, Hui and Tang, Xianfeng and Dai, Zhenwei and Han, Yan and Luo, Chen and Huang, Jing and Li, Zhen and others},
  abbr={ACL},
  journal={Association for Computational Linguistics},
  year={2025},
  abstract={Chain-of-Thought (CoT) reasoning, which
breaks down complex tasks into intermediate reasoning steps, has significantly enhanced
the performance of large language models
(LLMs) on challenging tasks. However, the
detailed reasoning process in CoT often incurs long generation times and high computational costs, partly due to the inclusion of
unnecessary steps. To address this, we propose a method to identify critical reasoning
steps using perplexity as a measure of their
importance: a step is deemed critical if its removal causes a significant increase in perplexity. Our method enables models to focus solely
on generating these critical steps. This can
be achieved through two approaches: refining demonstration examples in few-shot CoT
or fine-tuning the model using selected examples that include only critical steps. Comprehensive experiments validate the effectiveness
of our method, which achieves a better balance
between the reasoning accuracy and efficiency
of CoT.},
  pdf={https://arxiv.org/pdf/2502.13260}
}

@inproceedings{zeng2025towards,
  title={Towards Context-Robust LLMs: A Gated Representation Fine-tuning Approach},
  abbr={ACL},
  author={Zeng, Shenglai and He, Pengfei and Guo, Kai and Zheng, Tianqi and Lu, Hanqing and Xing, Yue and Liu, Hui},
  journal={Association for Computational Linguistics},
  year={2025},
  abstract={Large Language Models (LLMs) enhanced with external contexts, such as through retrieval-augmented generation (RAG), often face challenges in handling imperfect evidence. They tend to over-rely on external knowledge, making them vulnerable to misleading and unhelpful contexts. To address this, we propose the concept of context-robust LLMs, which can effectively balance internal knowledge with external context, similar to human cognitive processes. Specifically, context-robust LLMs should rely on external context only when lacking internal knowledge, identify contradictions between internal and external knowledge, and disregard unhelpful contexts. To achieve this goal, we introduce Grft, a lightweight and plug-and-play gated representation fine-tuning approach. Grft consists of two key components: a gating mechanism to detect and filter problematic inputs, and low-rank representation adapters to adjust hidden representations. By training a lightweight interv,ention function with only 0.0004\% of model size on fewer than 200 examples, Grft can effectively adapt LLMs towards context-robust behaviors.},
  pdf={https://arxiv.org/pdf/2502.14100}
}

@misc{dong2025practical,
  abbr={preprint},
  title={A practical memory injection attack against llm agents},
  author={Dong, Shen and Xu, Shaochen and He, Pengfei and Li, Yige and Tang, Jiliang and Liu, Tianming and Liu, Hui and Xiang, Zhen},
  journal={arXiv preprint arXiv:2503.03704},
  year={2025},
  abstract={Agents based on large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, that enables the injection of malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps leading to undesirable agent actions when executing the victim user's query. Specifically, we introduce a sequence of bridging steps to link the victim query to the malicious reasoning steps. During the injection of the malicious record, we propose an indication prompt to guide the agent to autonomously generate our designed bridging steps. We also propose a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing the victim query comes after. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting practical risks of LLM agents.},
  pdf={https://arxiv.org/pdf/2503.03704}
}

@misc{he2025attentionknowstrustattentionbased,
      abbr={preprint},
      title={Attention Knows Whom to Trust: Attention-based Trust Management for LLM Multi-Agent Systems}, 
      author={Pengfei He and Zhenwei Dai and Xianfeng Tang and Yue Xing and Hui Liu and Jingying Zeng and Qiankun Peng and Shrivats Agrawal and Samarth Varshney and Suhang Wang and Jiliang Tang and Qi He},
      year={2025},
      eprint={2506.02546},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.02546},
      abstract={Large Language Model-based Multi-Agent Systems (LLM-MAS) have demonstrated strong capabilities in solving complex tasks but remain vulnerable when agents receive unreliable messages. This vulnerability stems from a fundamental gap: LLM agents treat all incoming messages equally without evaluating their trustworthiness. While some existing studies approach the trustworthiness, they focus on a single type of harmfulness rather than analyze it in a holistic approach from multiple trustworthiness perspectives. In this work, we propose Attention Trust Score (A-Trust), a lightweight, attention-based method for evaluating message trustworthiness. Inspired by human communication literature[1], through systematically analyzing attention behaviors across six orthogonal trust dimensions, we find that certain attention heads in the LLM specialize in detecting specific types of violations. Leveraging these insights, A-Trust directly infers trustworthiness from internal attention patterns without requiring external prompts or verifiers. Building upon A-Trust, we develop a principled and efficient trust management system (TMS) for LLM-MAS, enabling both message-level and agent-level trust assessment. Experiments across diverse multi-agent settings and tasks demonstrate that applying our TMS significantly enhances robustness against malicious inputs.},
      pdf={https://arxiv.org/pdf/2506.02546}
}

@misc{he2025comprehensivevulnerabilityanalysisnecessary,
      abbr={preprint},
      title={Comprehensive Vulnerability Analysis is Necessary for Trustworthy LLM-MAS}, 
      author={Pengfei He and Yue Xing and Shen Dong and Juanhui Li and Zhenwei Dai and Xianfeng Tang and Hui Liu and Han Xu and Zhen Xiang and Charu C. Aggarwal and Hui Liu},
      year={2025},
      eprint={2506.01245},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.01245}, 
      abstract={This paper argues that a comprehensive vulnerability analysis is essential for building trustworthy Large Language Model-based Multi-Agent Systems (LLM-MAS). These systems, which consist of multiple LLM-powered agents working collaboratively, are increasingly deployed in high-stakes applications but face novel security threats due to their complex structures. While single-agent vulnerabilities are well-studied, LLM-MAS introduces unique attack surfaces through inter-agent communication, trust relationships, and tool integration that remain significantly underexplored. We present a systematic framework for vulnerability analysis of LLM-MAS that unifies diverse research. For each type of vulnerability, we define formal threat models grounded in practical attacker capabilities and illustrate them using real-world LLM-MAS applications. This formulation enables rigorous quantification of vulnerability across different architectures and provides a foundation for designing meaningful evaluation benchmarks. Our analysis reveals that LLM-MAS faces elevated risk due to compositional effects -- vulnerabilities in individual components can cascade through agent communication, creating threat models not present in single-agent systems. We conclude by identifying critical open challenges: (1) developing benchmarks specifically tailored to LLM-MAS vulnerability assessment, (2) considering new potential attacks specific to multi-agent architectures, and (3) implementing trust management systems that can enforce security in LLM-MAS. This research provides essential groundwork for future efforts to enhance LLM-MAS trustworthiness as these systems continue their expansion into critical applications.},
      pdf={https://arxiv.org/pdf/2506.01245}
}