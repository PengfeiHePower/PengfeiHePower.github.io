---
---

@string{aps = {ieee,}}



@inproceedings{10.1145/3511808.3557130,
author = {He, Pengfei and Liu, Haochen and Zhao, Xiangyu and Liu, Hui and Tang, Jiliang},
title = {PROPN: Personalized Probabilistic Strategic Parameter Optimization in Recommendations},
year = {2022},
isbn = {9781450392365},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3511808.3557130},
doi = {10.1145/3511808.3557130},
booktitle = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management (CIKM)},
pages = {3152–3161},
numpages = {10},
keywords = {recommender system, demographic information, reinforcement learning, strategic parameter optimizing},
location = {Atlanta, GA, USA},
series = {CIKM '22}
}

@article{JMLR:v24:21-1254,
  author  = {Nicolas Garcia Trillos and Pengfei He and Chenghui Li},
  title   = {Large sample spectral analysis of graph-based multi-manifold clustering},
  journal = {Journal of Machine Learning Research (JMLR)},
  year    = {2023},
  volume  = {24},
  number  = {143},
  pages   = {1--71},
  url     = {http://jmlr.org/papers/v24/21-1254.html},
  abstract = {In this work we study statistical properties of graph-based algorithms for multi-manifold
clustering (MMC). In MMC the goal is to retrieve the multi-manifold structure underlying a
given Euclidean data set when this one is assumed to be obtained by sampling a distribution
on a union of manifolds M = M1 ∪ · · · ∪ MN that may intersect with each other and
that may have different dimensions. We investigate sufficient conditions that similarity
graphs on data sets must satisfy in order for their corresponding graph Laplacians to
capture the right geometric information to solve the MMC problem. Precisely, we provide
high probability error bounds for the spectral approximation of a tensorized Laplacian on
M with a suitable graph Laplacian built from the observations; the recovered tensorized
Laplacian contains all geometric information of all the individual underlying manifolds. We
provide an example of a family of similarity graphs, which we call annular proximity graphs
with angle constraints, satisfying these sufficient conditions. We contrast our family of
graphs with other constructions in the literature based on the alignment of tangent planes.
Extensive numerical experiments expand the insights that our theory provides on the MMC
problem.},
  pdf = {http://jmlr.org/papers/v24/21-1254.html}
}

@inproceedings{xu2023probabilistic,
  title={Probabilistic Categorical Adversarial Attack and Adversarial Training},
  author={Xu, Han and He, Pengfei and Ren, Jie and Wan, Yuxuan and Liu, Zitao and Liu, Hui and Tang, Jiliang},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={38428--38442},
  year={2023},
  abstract={The studies on adversarial attacks and defenses
have greatly improved the robustness of Deep
Neural Networks (DNNs). Most advanced approaches have been overwhelmingly designed for
continuous data such as images. However, these
achievements are still hard to be generalized to
categorical data. To bridge this gap, we propose
a novel framework, Probabilistic Categorical Adversarial Attack (or PCAA). It transfers the discrete optimization problem of finding categorical
adversarial examples to a continuous problem that
can be solved via gradient-based methods. We analyze the optimality (attack success rate) and time
complexity of PCAA to demonstrate its significant advantage over current search-based attacks.
More importantly, through extensive empirical
studies, we demonstrate that the well-established
defenses for continuous data, such as adversarial
training and TRADES, can be easily accommodated to defend DNNs for categorical data},
  organization={PMLR},
  pdf = {https://proceedings.mlr.press/v202/xu23e.html}
}
@inproceedings{he2023sharpness,
  title={Sharpness-Aware Data Poisoning Attack},
  author={He, Pengfei and Xu, Han and Ren, Jie and Cui, Yingqian and Liu, Hui and Aggarwal, Charu C and Tang, Jiliang},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024},
  note={Spotlight Paper, 5%},
  abstract={Recent research has highlighted the vulnerability of Deep Neural Networks (DNNs)
against data poisoning attacks. These attacks aim to inject poisoning samples into
the models’ training dataset such that the trained models have inference failures.
While previous studies have executed different types of attacks, one major challenge
that greatly limits their effectiveness is the uncertainty of the re-training process
after the injection of poisoning samples, including the re-training initialization or
algorithms. To address this challenge, we propose a novel attack method called
“Sharpness-Aware Data Poisoning Attack (SAPA)”. In particular, it leverages the
concept of DNNs’ loss landscape sharpness to optimize the poisoning effect on the
worst re-trained model. It helps enhance the preservation of the poisoning effect,
regardless of the specific retraining procedure employed. Extensive experiments
demonstrate that SAPA offers a general and principled strategy that significantly
enhances various types of poisoning attacks.},
  pdf = {/assets/pdf/Sharpness_aware_poisoning_attacks.pdf}
}

@misc{cui2023diffusionshield,
  title={DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models},
  author={Cui, Yingqian and Ren, Jie and Xu, Han and He, Pengfei and Liu, Hui and Sun, Lichao and Tang, Jiliang},
  journal={arXiv preprint arXiv:2306.04642},
  year={2023}
}

@article{bjarnadottir2023analyzing,
  title={Analyzing Illegal Psychostimulant Trafficking Networks Using Noisy and Sparse Data},
  author={Bjarnadottir, Margret V and Chandra, Siddharth and He, Pengfei and Midgette, Greg},
  journal={IISE Transactions},
  number={just-accepted},
  pages={1--20},
  year={2023},
  publisher={Taylor \& Francis}
}

@inproceedings{zeng-etal-2024-good,
    title = "The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented Generation ({RAG})",
    author = "Zeng, Shenglai  and
      Zhang, Jiankun  and
      He, Pengfei  and
      Liu, Yiding  and
      Xing, Yue  and
      Xu, Han  and
      Ren, Jie  and
      Chang, Yi  and
      Wang, Shuaiqiang  and
      Yin, Dawei  and
      Tang, Jiliang",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand and virtual meeting",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.267",
    doi = "10.18653/v1/2024.findings-acl.267",
    pages = "4505--4524",
    abstract = "Retrieval-augmented generation (RAG) is a powerful technique to facilitate language model generation with proprietary and private data, where data privacy is a pivotal concern. Whereas extensive research has demonstrated the privacy risks of large language models (LLMs), the RAG technique could potentially reshape the inherent behaviors of LLM generation, posing new privacy issues that are currently under-explored. To this end, we conduct extensive empirical studies with novel attack methods, which demonstrate the vulnerability of RAG systems on leaking the private retrieval database. Despite the new risks brought by RAG on the retrieval data, we further discover that RAG can be used to mitigate the old risks, i.e., the leakage of the LLMs{'} training data. In general, we reveal many new insights in this paper for privacy protection of retrieval-augmented LLMs, which could benefit both LLMs and RAG systems builders.",
}

@misc{he2023confidencedriven,
      title={Confidence-driven Sampling for Backdoor Attacks}, 
      author={Pengfei He and Han Xu and Yue Xing and Jie Ren and Yingqian Cui and Shenglai Zeng and Jiliang Tang and Makoto Yamada and Mohammad Sabokrou},
      year={2023},
      eprint={2310.05263},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{cui2023ftshield,
      title={FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models}, 
      author={Yingqian Cui and Jie Ren and Yuping Lin and Han Xu and Pengfei He and Yue Xing and Wenqi Fan and Hui Liu and Jiliang Tang},
      year={2023},
      eprint={2310.02401},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{zeng-etal-2024-exploring,
    title = "Exploring Memorization in Fine-tuned Language Models",
    author = "Zeng, Shenglai  and
      Li, Yaxin  and
      Ren, Jie  and
      Liu, Yiding  and
      Xu, Han  and
      He, Pengfei  and
      Xing, Yue  and
      Wang, Shuaiqiang  and
      Tang, Jiliang  and
      Yin, Dawei",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.216",
    doi = "10.18653/v1/2024.acl-long.216",
    pages = "3917--3948",
    abstract = "Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models{'} (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution.",
}

@inproceedings{xu2023generalization,
    title = "On the Generalization of Training-based ChatGPT Detection Methods",
    author = "Han Xu and Jie Ren and Pengfei He and Shenglai Zeng and Yingqian Cui and Amy Liu and Hui Liu and Jiliang Tang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "ChatGPT is one of the most popular language models which achieve amazing performance on various natural language tasks. Consequently, there is also an urgent need to detect the texts generated ChatGPT from human written. One of the extensively studied methods trains classification models to distinguish both. However, existing studies also demonstrate that the trained models may suffer from distribution shifts (during test), i.e., they are ineffective to predict the generated texts from unseen language tasks or topics. In this work, we aim to have a comprehensive investigation on these methods' generalization behaviors under distribution shift caused by a wide range of factors, including prompts, text lengths, topics, and language tasks. To achieve this goal, we first collect a new dataset with human and ChatGPT texts, and then we conduct extensive studies on the collected dataset. Our studies unveil insightful findings which provide guidance for developing future methodologies or data collection strategies for ChatGPT detection.",
}

@inproceedings{lin2024towards,
    title = "Towards Understanding Jailbreak Attacks in LLMs: A Representation Space Analysis",
    author = "He, Pengfei and Lin, Yuping and Xu, Han and Xing, Yue and Yamada, Makoto and Liu, Hui and Tang, Jiliang",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2024",
    publisher = "Association for Computational Linguistics",
    abstract = "Large language models (LLMs) are susceptible to a type of attack known as jailbreaking, which misleads LLMs to output harmful contents. Although there are diverse jailbreak attack strategies, there is no unified understanding on why some methods succeed and others fail. This paper explores the behavior of harmful and harmless prompts in the LLM's representation space to investigate the intrinsic properties of successful jailbreak attacks. We hypothesize that successful attacks share some similar properties: They are effective in moving the representation of the harmful prompt towards the direction to the harmless prompts. We leverage hidden representations into the objective of existing jailbreak attacks to move the attacks along the acceptance direction, and conduct experiments to validate the above hypothesis using the proposed objective. We hope this study provides new insights into understanding how LLMs understand harmfulness information.",
}


@misc{he2024data,
      title={Data Poisoning for In-context Learning}, 
      author={Pengfei He and Han Xu and Yue Xing and Hui Liu and Makoto Yamada and Jiliang Tang},
      year={2024},
      eprint={2402.02160},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      abstract={In the domain of large language models (LLMs), in-context 
      learning (ICL) has been recognized for its innovative ability to adapt to new tasks, 
      relying on examples rather than retraining or fine-tuning. This paper delves into 
      the critical issue of ICL's susceptibility to data poisoning attacks, an area not 
      yet fully explored. We wonder whether ICL is vulnerable, with adversaries capable 
      of manipulating example data to degrade model performance. To address this, 
      we introduce ICLPoison, a specialized attacking framework conceived to exploit 
      the learning mechanisms of ICL. Our approach uniquely employs discrete text 
      perturbations to strategically influence the hidden states of LLMs during the 
      ICL process. We outline three representative strategies to implement attacks 
      under our framework, each rigorously evaluated across a variety of models and 
      tasks. Our comprehensive tests, including trials on the sophisticated GPT-4 model, 
      demonstrate that ICL's performance is significantly compromised under our framework. 
      These revelations indicate an urgent need for enhanced defense mechanisms to safeguard 
      the integrity and reliability of LLMs in applications relying on in-context learning.},
      pdf={https://arxiv.org/pdf/2402.02160.pdf}
}

@misc{ren2024copyright,
      title={Copyright Protection in Generative AI: A Technical Perspective}, 
      author={Jie Ren and Han Xu and Pengfei He and Yingqian Cui and Shenglai Zeng and Jiankun Zhang and Hongzhi Wen and Jiayuan Ding and Hui Liu and Yi Chang and Jiliang Tang},
      year={2024},
      eprint={2402.02333},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{cui2024superiority,
      title={Superiority of Multi-Head Attention in In-Context Linear Regression}, 
      author={Yingqian Cui and Jie Ren and Pengfei He and Jiliang Tang and Yue Xing},
      year={2024},
      eprint={2401.17426},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{he2024towards,
  title={Towards the Effect of Examples on In-Context Learning: A Theoretical Case Study},
  author={He, Pengfei and Cui, Yingqian and Xu, Han and Liu, Hui and Yamada, Makoto and Tang, Jiliang and Xing, Yue},
  journal={M3L & SFLLM workshop NeruIPS 2024},
  year={2024}
}